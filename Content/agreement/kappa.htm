<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd" MadCap:lastBlockDepth="4" MadCap:lastHeight="4150.104" MadCap:lastWidth="2828">
    <head><title>Agreement Analysis (Categorical Data, Kappa, Maxwell, Scott Pi, Gwet AC1) - StatsDirect</title>
        <link rel="StyleSheet" href="../resources/Stylesheets/STATSDIRECT.css" type="text/css" />
        <script type="text/javascript">
        </script>
        <meta name="description" content="For the case of two raters, this function gives Cohen's kappa (weighted and unweighted), Scott's pi and Gwett's AC1 as measures of inter-rater agreement for two raters' categorical assessments. For three or more raters, this function gives extensions of the Cohen kappa method, due to Fleiss and Cuzick in the case of two possible responses per rater, and Fleiss, Nee and Landis in the general case of three or more responses per rater." />
    </head>
    <body>
        <h1>
            <MadCap:keyword term="Inter-rater agreement: Kappa;Stuart-Maxwell;Scott-Cohen pi;Reliability: Categorical;Agreement analysis: Categorical;Kappa;Maxwell;Gwet AC1" />Agreement of Categorical Measurements</h1>
        <p>&#160;</p>
        <p>Menu locations:<br /><b>Analysis_Agreement_Categorical (Kappa)</b><br /><b>Analysis_Chi-square_Kappa and Maxwell</b><br /><b>Analysis_Clinical Epidemiology_Kappa and Maxwell</b></p>
        <p>.</p>
        <p><u>Agreement Analysis</u>
        </p>
        <p>
            <MadCap:keyword term="Cohen's kappa" />For the case of two raters, this function gives Cohen's kappa (weighted and unweighted), Scott's pi and Gwett's AC1 as measures of inter-rater agreement for two raters' categorical assessments (<a href="../references/reference_list.htm">Fleiss, 1981; Fleiss, 1969; Altman, 1991; Scott 1955</a>). For three or more raters, this function gives extensions of the Cohen kappa method, due to <a href="../references/reference_list.htm">Fleiss and Cuzick (1979)</a> in the case of two possible responses per rater, and <a href="../references/reference_list.htm">Fleiss, Nee and Landis (1979)</a> in the general case of three or more responses per rater.</p>
        <p>&#160;</p>
        <p>If you have only two categories then Scott's pi statistic (with confidence interval constructed by the <a href="../references/reference_list.htm">Donner-Eliasziw (1992)</a> method) for inter-rater agreement (<a href="../references/reference_list.htm">Zwick, 1988</a>) is more reliable than kappa.</p>
        <p>&#160;</p>
        <p>Gwet's AC1 is the statistic of choice for the case of two raters (<a href="../references/reference_list.htm">Gwet, 2008</a>). Gwet's agreement coefficient, can be used in more contexts than kappa or pi because it does not depend upon the assumption of independence between raters.</p>
        <p>&#160;</p>
        <p>Weighted kappa partly compensates for a problem with unweighted kappa, namely that it is not adjusted for the degree of disagreement. Disagreement is weighted in decreasing priority from the top left (origin) of the table. StatsDirect uses the following definitions for weight (1 is the default):</p>
        <ol>
            <li>w(ij)=1-abs(i-j)/(g-1)</li>
            <li>w(ij)=1-[(i-j)/(g-1)]²</li>
            <li>User defined (this is only available via workbook data entry)</li>
        </ol>
        <p>g = categories</p>
        <p>w = weight</p>
        <p>i = category for one observer (from 1 to g)</p>
        <p>j = category for the other observer (from 1 to g)</p>
        <p>&#160;</p>
        <p>In broad terms a kappa below 0.2 indicates poor agreement and a kappa above 0.8 indicates very good agreement beyond chance.</p>
        <p>&#160;</p>
        <p>Guide (<a href="../references/reference_list.htm">Landis and Koch, 1977</a>):</p>
        <table cellspacing="0" style="margin-left: 0;margin-right: auto;caption-side: top;border-spacing: 2px 2px;border-collapse: separate;">
            <col />
            <col />
            <tr>
                <td style="padding-left: 0em;padding-right: 2em;border-left-width: .5em;border-right-width: .5em;border-top-width: .5em;border-bottom-width: .5em;"><u>Kappa</u>
                </td>
                <td style="text-decoration: underline;padding-left: 0em;padding-right: 2em;border-left-width: .5em;border-right-width: .5em;border-top-width: .5em;border-bottom-width: .5em;">Strength of agreement</td>
            </tr>
            <tr>
                <td style="padding-left: 0em;padding-right: 2em;border-left-width: .5em;border-right-width: .5em;border-top-width: .5em;border-bottom-width: .5em;">&lt; 0.2</td>
                <td style="padding-left: 0em;padding-right: 2em;border-left-width: .5em;border-right-width: .5em;border-top-width: .5em;border-bottom-width: .5em;">Poor</td>
            </tr>
            <tr>
                <td style="padding-left: 0em;padding-right: 2em;border-left-width: .5em;border-right-width: .5em;border-top-width: .5em;border-bottom-width: .5em;">&gt; 0.2 ≤ 0.4</td>
                <td style="padding-left: 0em;padding-right: 2em;border-left-width: .5em;border-right-width: .5em;border-top-width: .5em;border-bottom-width: .5em;">Fair</td>
            </tr>
            <tr>
                <td style="padding-left: 0em;padding-right: 2em;border-left-width: .5em;border-right-width: .5em;border-top-width: .5em;border-bottom-width: .5em;">&gt; 0.4 ≤ 0.6</td>
                <td style="padding-left: 0em;padding-right: 2em;border-left-width: .5em;border-right-width: .5em;border-top-width: .5em;border-bottom-width: .5em;">Moderate</td>
            </tr>
            <tr>
                <td style="padding-left: 0em;padding-right: 2em;border-left-width: .5em;border-right-width: .5em;border-top-width: .5em;border-bottom-width: .5em;">&gt; 0.6 ≤ 0.8</td>
                <td style="padding-left: 0em;padding-right: 2em;border-left-width: .5em;border-right-width: .5em;border-top-width: .5em;border-bottom-width: .5em;">Good</td>
            </tr>
            <tr>
                <td style="padding-left: 0em;padding-right: 2em;border-left-width: .5em;border-right-width: .5em;border-top-width: .5em;border-bottom-width: .5em;">&gt; 0.8 ≤ 1</td>
                <td style="padding-left: 0em;padding-right: 2em;border-left-width: .5em;border-right-width: .5em;border-top-width: .5em;border-bottom-width: .5em;">Very good</td>
            </tr>
        </table>
        <p>&#160;</p>
        <p>N.B. You can not reliably compare kappa values from different studies because kappa is sensitive to the prevalence of different categories. i.e. if one category is observed more commonly in one study than another then kappa may indicate a difference in inter-rater agreement which is not due to the raters.</p>
        <p>&#160;</p>
        <p>Agreement analysis with more than two raters is a complex and controversial subject, see <a href="../references/reference_list.htm">Fleiss (1981, p. 225)</a>.</p>
        <p>&#160;</p>
        <p><u>Disagreement Analysis</u>
        </p>
        <p>StatsDirect uses the methods of <a href="../references/reference_list.htm">Maxwell (1970)</a> to test for differences between the ratings of the two raters (or k nominal responses with paired observations).</p>
        <p>&#160;</p>
        <p>
            <MadCap:keyword term="McNemar" />Maxwell's chi-square statistic tests for overall disagreement between the two raters. The general McNemar statistic tests for asymmetry in the distribution of subjects about which the raters disagree, i.e. disagreement more over some categories of response than others.</p>
        <p>&#160;</p>
        <p>Data preparation</p>
        <p>You may present your data for the two-rater methods as a fourfold table in the interactive screen data entry menu option. Otherwise, you may present your data as responses/ratings in columns and rows in a worksheet, where the columns represent raters and the rows represent subjects rated. If you have more than two raters then you must present your data in the worksheet column (rater) row (subject) format. Missing data can be used where raters did not rate all subjects.</p>
        <p>&#160;</p>
        <p><u>Technical validation</u>
        </p>
        <p>All formulae for kappa statistics and their tests are as per <a href="../references/reference_list.htm">Fleiss (1981)</a>:</p>
        <p>For two raters (m=2) and two categories (k=2):</p>
        <p>
            <MadCap:equation>$\displaystyle p_o=\sum_{i=1}^k \sum_{j=1}^k w_{ij} \space p_{ij}$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\displaystyle p_e=\sum_{i=1}^k \sum_{j=1}^k w_{ij} \space p_{i.} \space p_{.j}$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\displaystyle \hat{\kappa} = (p_o-p_e)/(1-p_e)$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\displaystyle s_o=\frac{1}{(1-p_e) \space \sqrt{n}} \sqrt{[\sum_{i=1}^k\sum_{j=1}^k p_{i.} \space p_{.j} \space (\bar{w}_{ij}-(\bar{w}_{i.}+\bar{w}_{.i}))]-p_e^2}$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\displaystyle p_{i.}=\sum_{j=1}^k p_{ij}$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\displaystyle p_{.j}=\sum_{i=1}^k p_{ij}$</MadCap:equation>
        </p>
        <p>- where n is the number of subjects rated, w is the weight for agreement or disagreement, p<sub>o</sub> is the observed proportion of agreement, p<sub>e</sub> is the expected proportion of agreement, p<sub>ij</sub> is the fraction of ratings i by the first rater and j by the second rater, and so is the standard error for testing that the kappa statistic equals zero.</p>
        <p>&#160;</p>
        <p>For three or more raters (m&gt;2) and two categories (k =2):</p>
        <p>
            <MadCap:equation>$\displaystyle B=\frac{1}{n}\sum_{i=1}^k{\frac{(x_i-m_i \bar{p})^2}{m_i}}$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\displaystyle W=\frac{1}{n(\bar{m}-1)} \sum_{i=1}^k{\frac{x_i(m_i-x_i)}{m_i}}$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\displaystyle \hat{\kappa}=\frac{B-W}{B+(\bar{m}-1)W}$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\displaystyle s_o=\frac{1}{(\bar{m}-1)\sqrt{n \bar{m}_h}} \sqrt{2(\bar{m}_h-1)+\frac{(\bar{m}-\bar{m}_h)(1-4\bar{p}\bar{q})}{\bar{m}\bar{p}\bar{q}}}$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\displaystyle \bar{p}=\sum_{i=1}^k \frac{x_i}{n\bar{m}}$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\displaystyle \bar{m}=\sum_{i=1}^k{\frac{m_i}{n}}$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\displaystyle \bar{m}_h=\frac{k}{\sum_{i=1}^k{1/m_i}}$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\bar{q}=1-\bar{p}$</MadCap:equation>
        </p>
        <p>- where x<sub>i</sub> is the number of positive ratings out of m<sub>i</sub> raters for subject i of n subjects, and so is the standard error for testing that the kappa statistic equals zero.</p>
        <p>&#160;</p>
        <p>For three or more raters and categories (m&gt;2, k&gt;2):</p>
        <p>
            <MadCap:equation>$\displaystyle \bar{\kappa}=\frac{\displaystyle \sum_{j=1}^k\bar{p}_j \bar{q}_j \hat{\kappa}_j}{\displaystyle \sum_{j=1}^k{\bar{p}_j \bar{q}_j}}$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\displaystyle s_{oj}=\sqrt{\frac{2}{nm(m-1)}}$</MadCap:equation>
        </p>
        <p>
            <MadCap:equation>$\displaystyle \bar{s}_o=\frac{\sqrt{2}}{\displaystyle \sum_{i=1}^k{\bar{p}_j \bar{q}_j\sqrt{nm(m-1)}}} \sqrt{[\sum_{j=1}^k{\bar{p}_j\bar{q}_j}]^2-\sum_{j=1}^k{\bar{p}_j\bar{q}_j(\bar{q}_j-\bar{p}_j)}}$</MadCap:equation>
        </p>
        <p>- where s<sub>oj</sub> is the standard error for testing kappa equal for each rating category separately, and so bar is the standard error for testing kappa equal to zero for the overall kappa across the k categories. Kappa hat is calculated as for the m&gt;2, k=2 method shown above.</p>
        <p>&#160;</p>
        <p><u>Example</u>
        </p>
        <p>From <a href="../references/reference_list.htm">Altman (1991)</a>.</p>
        <p>&#160;</p>
        <p>Altman quotes the results of Brostoff et al. in a comparison not of two human observers but of two different methods of assessment. These methods are RAST (radioallergosorbent test) and MAST (multi-RAST) for testing the sera of individuals for specifically reactive IgE in the diagnosis of allergies. Five categories of result were recorded using each method:</p>
        <p>&#160;</p>
        <table cellspacing="0" style="caption-side: top;">
            <col />
            <col />
            <col />
            <col />
            <col />
            <col />
            <col />
            <tr>
                <td width="54">&#160;</td>
                <td width="72">&#160;</td>
                <td colspan="5" width="352" style="text-align: center;">RAST:</td>
            </tr>
            <tr>
                <td width="54">&#160;</td>
                <td width="72">&#160;</td>
                <td width="69" style="text-decoration: underline;text-align: right;">negative</td>
                <td width="67" style="text-decoration: underline;text-align: right;">weak</td>
                <td width="73" style="text-decoration: underline;text-align: right;">moderate</td>
                <td width="67" style="text-decoration: underline;text-align: right;">high</td>
                <td width="76" style="text-decoration: underline;text-align: right;">very high</td>
            </tr>
            <tr>
                <td rowspan="5" width="54">MAST:</td>
                <td width="72">negative</td>
                <td width="69" style="text-align: right;">86</td>
                <td width="67" style="text-align: right;">3</td>
                <td width="73" style="text-align: right;">14</td>
                <td width="67" style="text-align: right;">0</td>
                <td width="76" style="text-align: right;">2</td>
            </tr>
            <tr>
                <td width="72">weak</td>
                <td width="69" style="text-align: right;">26</td>
                <td width="67" style="text-align: right;">0</td>
                <td width="73" style="text-align: right;">10</td>
                <td width="67" style="text-align: right;">4</td>
                <td width="76" style="text-align: right;">0</td>
            </tr>
            <tr>
                <td width="72">moderate</td>
                <td width="69" style="text-align: right;">20</td>
                <td width="67" style="text-align: right;">2</td>
                <td width="73" style="text-align: right;">22</td>
                <td width="67" style="text-align: right;">4</td>
                <td width="76" style="text-align: right;">1</td>
            </tr>
            <tr>
                <td width="72">high</td>
                <td width="69" style="text-align: right;">11</td>
                <td width="67" style="text-align: right;">1</td>
                <td width="73" style="text-align: right;">37</td>
                <td width="67" style="text-align: right;">16</td>
                <td width="76" style="text-align: right;">14</td>
            </tr>
            <tr>
                <td width="72">very high</td>
                <td width="69" style="text-align: right;">3</td>
                <td width="67" style="text-align: right;">0</td>
                <td width="73" style="text-align: right;">15</td>
                <td width="67" style="text-align: right;">24</td>
                <td width="76" style="text-align: right;">48</td>
            </tr>
        </table>
        <p>&#160;</p>
        <p>To analyse these data in StatsDirect select Categorical from the Agreement section of the Analysis menu. Choose the default 95% confidence interval. Enter the above frequencies as directed on the screen and select the default method for weighting.</p>
        <p>&#160;</p>
        <p>For this example:</p>
        <p>&#160;</p>
        <p><u>General agreement over all categories (2 raters)</u>
        </p>
        <p>&#160;</p>
        <p style="text-decoration: underline;">Cohen's kappa (unweighted)</p>
        <p>Observed agreement = 47.38%</p>
        <p>Expected agreement = 22.78%</p>
        <p>Kappa = 0.318628 (se_0 = 0.026776, se = 0.030423)</p>
        <p>95% confidence interval = 0.259 to 0.378256</p>
        <p>z (for k = 0) = 11.899574</p>
        <p>P &lt; 0.0001</p>
        <p>&#160;</p>
        <p style="text-decoration: underline;">Cohen's kappa (weighted by 1-Abs(i-j)/(1 - k))</p>
        <p>Observed agreement = 80.51%</p>
        <p>Expected agreement = 55.81%</p>
        <p>Kappa = 0.558953 (se_0 = 0.038019, se = 0.028507)</p>
        <p>95% confidence interval = 0.503081 to 0.614826</p>
        <p>z (for kw = 0) = 14.701958</p>
        <p>P &lt; 0.0001</p>
        <p>&#160;</p>
        <p style="text-decoration: underline;">Scott's pi</p>
        <p>Observed agreement = 47.38%</p>
        <p>Expected agreement = 24.07%</p>
        <p>Pi = 0.30701</p>
        <p>&#160;</p>
        <p style="text-decoration: underline;">Gwet's AC1</p>
        <p>Observed agreement = 35.06%</p>
        <p>Chance-independent agreement = 18.98%</p>
        <p>AC1 = 0.350552 (se = 0.033046)</p>
        <p>95% confidence interval = 0.285782 to 0.415322</p>
        <p>&#160;</p>
        <p style="text-decoration: underline;">Disagreement over any category and asymmetry of disagreement (2 raters)</p>
        <p>Marginal homogeneity (Maxwell) chi-square = 73.013451, df = 4, P &lt; 0.0001</p>
        <p>Symmetry (generalised McNemar) chi-square = 79.076091, df = 10, P &lt; 0.0001</p>
        <p>&#160;</p>
        <p>Note that for calculation of standard errors for the kappa statistics, StatsDirect uses a more accurate method than that which is quoted in most textbooks (e.g. <a href="../references/reference_list.htm">Altman, 1990</a>).</p>
        <p>&#160;</p>
        <p>The statistically highly significant z tests indicate that we should reject the null hypothesis that the ratings are independent (i.e. kappa = 0) and accept the alternative that agreement is better than one would expect by chance. Do not put too much emphasis on the kappa statistic test, it makes a lot of assumptions and falls into error with small numbers.</p>
        <p>&#160;</p>
        <p>Note that Gwet's agreement coefficient does not depend upon the hypothesis of independence between raters, therefore you can use it to reflect the extent of agreement in more contexts than kappa.</p>
        <p>&#160;</p>
        <p>The statistically highly significant Maxwell test statistic above indicates that the raters disagree significantly in at least one category. The generalised McNemar statistic indicates the disagreement is not spread evenly.</p>
        <p>&#160;</p>
        <p><a href="../basics/confidence_interval.htm">confidence intervals</a>
        </p>
        <p><a href="../basics/p_values.htm">P values</a>
        </p>
    </body>
</html>